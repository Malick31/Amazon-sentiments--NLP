{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "class Sentiment:\n",
    "    Negative = \"Négative\"\n",
    "    Moyen = \"Moyen\"\n",
    "    Positive = \"Positive\"\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, texte, score):\n",
    "        self.texte = texte\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment()\n",
    "    def get_sentiment(self):\n",
    "        if self.score <= 2:\n",
    "            return Sentiment.Negative\n",
    "        elif self.score >= 4:\n",
    "            return Sentiment.Positive\n",
    "        else: \n",
    "            return Sentiment.Moyen\n",
    "\n",
    "\n",
    "\n",
    "class ReviewCont: \n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "    def get_txt(self):\n",
    "        return [x.texte for x in self.reviews]\n",
    "\n",
    "    def get_sent(self):\n",
    "        \n",
    "        \n",
    "        return [x.sentiment for x in self.reviews]\n",
    "        \n",
    "    \n",
    "    def distribution(self):\n",
    "        negative = list(filter (lambda x: x.sentiment == Sentiment.Negative, self.reviews))\n",
    "        positive = list(filter (lambda x: x.sentiment == Sentiment.Positive, self.reviews))\n",
    "        positive_ = positive[:len(negative)]\n",
    "        self.reviews = negative + positive_\n",
    "        random.shuffle(self.reviews)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hoped for Mia to have some peace in this book, but her story is so real and raw.  Broken World was so touching and emotional because you go from Mia\\'s trauma to her trying to cope.  I love the way the story displays how there is no \"just bouncing back\" from being sexually assaulted.  Mia showed us how those demons come for you every day and how sometimes they best you. I was so in the moment with Broken World and hurt with Mia because she was surrounded by people but so alone and I understood her feelings.  I found myself wishing I could give her some of my courage and strength or even just to be there for her.  Thank you Lizzy for putting a great character\\'s voice on a strong subject and making it so that other peoples story may be heard through Mia\\'s.'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = r\"C:\\Users\\malic\\Documents\\Auto-didact-Data\\Books_small_10000.json\"\n",
    "\n",
    "reviews = []\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        \n",
    "        review = json.loads(line)\n",
    "        reviews.append(Review(review['reviewText'],review['overall']))\n",
    "reviews[5].texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(r\"C:\\Users\\DeePak\\Desktop\\myac.csv\")\n",
    "# ou écrire directement le nom du fichier \n",
    "#\"C:/Users/DeePak/Desktop/myac.csv\")\n",
    "#(\"C:\\\\Users\\\\DeePak\\\\Desktop\\\\myac.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review:\n",
    "    def __init__(self, texte, score):\n",
    "        self.texte = texte\n",
    "        self.score = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(reviews, test_size=0.33, random_state=42)\n",
    "\n",
    "train_cont = ReviewCont(train)\n",
    "test_cont = ReviewCont(test)\n",
    "#train_cont.distribution()\n",
    "\n",
    "#len(cont.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6700"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "#train_x = [x.texte for x in train]\n",
    "#y_train = [x.sentiment for x in train]\n",
    "train_cont.distribution()\n",
    "train_x = train_cont.get_txt()\n",
    "y_train = train_cont.get_sent()\n",
    "\n",
    "test_cont.distribution()\n",
    "test_x = test_cont.get_txt()\n",
    "y_test = test_cont.get_sent()\n",
    "\n",
    "print(y_train.count(Sentiment.Positive))\n",
    "\n",
    "print(y_train.count(Sentiment.Negative))\n",
    "\n",
    "\n",
    "#test_x = [x.texte for x in test]\n",
    "#y_test = [x.sentiment for x in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I read this in one sitting- I have always loved Paris, but so much of this resonated. The people. The places, the laughs, the love. There is some great advice for people, enough travelogue to make you pack your bag. Sumptuous moments to savor the food morsels, and a love story to warm your heart. Oh the possibilities. !\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#count = CountVectorizer()\n",
    "count = TfidfVectorizer()\n",
    "X_train = count.fit_transform(train_x)\n",
    "#print(count.get_feature_names())\n",
    "print(train_x[0])\n",
    "print(X_train[0].toarray())\n",
    "X_test = count.transform(test_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"SVC\" : SVC(kernel='linear', random_state = 42),\n",
    "    \"DTC\" : DecisionTreeClassifier(random_state = 42),\n",
    "    \n",
    "    #\"NV\" : GaussianNB(),\n",
    "    \"RFC\": RandomForestClassifier(random_state = 42),\n",
    "    \"LR\" : LogisticRegression(random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test,y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    if False :\n",
    "        return acc\n",
    "    else: \n",
    "        print(f'Le score est :', {acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy(y_test,y_pred)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print( f1_score(y_test, y_pred, average=None, labels = [Sentiment.Positive, Sentiment.Negative\n",
    "                                                            ,Sentiment.Moyen]) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC :\n",
      "Le score est : {0.8076923076923077}\n",
      "[[170  38]\n",
      " [ 42 166]]\n",
      "[0.80582524 0.80952381 0.        ]\n",
      "DTC :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malic\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\malic\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score est : {0.6370192307692307}\n",
      "[[136  72]\n",
      " [ 79 129]]\n",
      "[0.63080685 0.643026   0.        ]\n",
      "RFC :\n",
      "Le score est : {0.6995192307692307}\n",
      "[[161  47]\n",
      " [ 78 130]]\n",
      "[0.67532468 0.72035794 0.        ]\n",
      "LR :\n",
      "Le score est : {0.8028846153846154}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malic\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\malic\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\malic\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\malic\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\malic\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\malic\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[169  39]\n",
      " [ 43 165]]\n",
      "[0.80097087 0.8047619  0.        ]\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    print(name,':')\n",
    "    evaluation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.count(Sentiment.Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ['I enjoy this, 5 stars',\" great\", \"bad look do not buy\", 'Horrible waste of time']\n",
    "new_test = count.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Positive', 'Négative', 'Négative'], dtype='<U8')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open (r\"C:\\Users\\malic\\Documents\\Auto-didact-Data\\sentiments_classifier.pkl\",'wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (r\"C:\\Users\\malic\\Documents\\Auto-didact-Data\\sentiments_classifier.pkl\", 'rb') as f:\n",
    "    load = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive'], dtype='<U8')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
